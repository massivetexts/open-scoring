{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/massivetexts/open-scoring/blob/master/notebooks/LLM-AUT-Scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0swIzCjnuF9g"
      },
      "source": [
        "# Using Open Creativity Scoring for Large Language Model Scoring\n",
        "\n",
        "*To run this code, use the play buttons to the left of the code cells/*\n",
        "\n",
        "First, install the OCS library and import it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-OQATof8uF9i"
      },
      "outputs": [],
      "source": [
        "!pip install -qq Open-Creativity-Scoring\n",
        "import open_scoring as ocs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRsiybp9uF9j"
      },
      "source": [
        "## Load a GPT-3 Scorer\n",
        "\n",
        "GPT-3 is a text mining architecture provided by OpenAI - our finetuned models are hosted on their systems and accessed through an API. The benefit of this is accessibility - you don't need fancy systems to run the large language model, because *they* have the model on a fancy system that you can talk to. The downside is that there are costs involve to using it (between `450` and `34000` scored responses for a dollar depending on which model is used), and you need an account with OpenAI.\n",
        "\n",
        "### Creating an API Key\n",
        "\n",
        "1. Sign up for an [account with openai.com](https://beta.openai.com/signup), and link a payment method. You can set a hard-limit on spending, if need be.\n",
        "2. Make an API Key here: https://beta.openai.com/account/api-keys\n",
        "3. Run the code below, and paste your API key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gXJKsguRuF9j"
      },
      "outputs": [],
      "source": [
        "scorer = ocs.scoring.GPT_Scorer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80kW1ic1uF9j"
      },
      "source": [
        "Here's how you use the scorer (it defaults to the *ada* model):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XuJkzO-NuF9k",
        "outputId": "4a915c3b-8f87-45dd-c448-5776bc6d1845"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.3"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scorer.originality(target='brick', response='paperweight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9Tsa2EUuF9k"
      },
      "source": [
        "Or simply:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KQ-D4lv3uF9k",
        "outputId": "9f388c42-a8f2-47f9-8de9-bffe19eac186"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scorer.originality('brick', 'use for a clock pendulum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5DRmZxRuF9l"
      },
      "source": [
        "## Using a different model\n",
        "\n",
        "A number of models are included and pre-trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrSZ_gJPuF9l",
        "outputId": "f24c8c80-06ab-4001-faa3-73636572cea8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ada', 'babbage', 'curie', 'davinci']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scorer.models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMJI8XcBuF9l"
      },
      "source": [
        "To use a difference model, supply a `model` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JTXKzhBuF9l",
        "outputId": "56cecd16-9a67-4102-d469-2ac91f662e65"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scorer.originality('brick', 'use for a clock pendulum', model='babbage')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIpspW_huF9m"
      },
      "source": [
        "Bigger models are costlier. Davinci works best, but can score only about `450` responses for a dollar, where ada can score 34000 and babbage can score about 23000. 'babbage' or 'curie' are a good trade-off in cost vs performance.\n",
        "\n",
        "Prices are listed at https://openai.com/api/pricing/, under \"Fine-tuned models\" > \"Usage\". Here is howhow many responses could be scored for a dollar, based on past studies performed in Summer 2022:\n",
        "\n",
        "| model        |   responses/dollar |\n",
        "|:-------------|-------------------:|\n",
        "| gpt3-ada     |              33966 |\n",
        "| gpt3-babbage |              22644 |\n",
        "| gpt3-curie   |               4529 |\n",
        "| gpt3-davinci |                453 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tnwpE54uF9m"
      },
      "source": [
        "## Scoring Many Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G972ZUANuF9m"
      },
      "source": [
        "Here's how to score many responses at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lpZoKo8GuF9m",
        "outputId": "206ddc51-1288-486d-dee6-5252df78ef9e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.16it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.5, 3.3]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scorer.originality_batch(['brick', 'rope'], ['use as a paperweight', 'dip the end in sugar and use to lure a raccoon closer to you'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FWpHmTTuF9m"
      },
      "source": [
        "If you're working from a DataFrame of data - a popular data science structure in Python - here's how you might score as a batch. First, I'll create a sample DataFrame. In a real-world setting you might load this data, with a function like [`pd.read_excel`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_excel.html) or [`pd.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6psVEojluF9m",
        "outputId": "6feb5cd1-2096-4dcf-9b6b-acf5b134d225"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>brick</td>\n",
              "      <td>use as a paperweight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rope</td>\n",
              "      <td>dip the end in sugar and use to lure a raccoon...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  prompt                                           response\n",
              "0  brick                               use as a paperweight\n",
              "1   rope  dip the end in sugar and use to lure a raccoon..."
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is just sample data\n",
        "import pandas as pd\n",
        "df = pd.DataFrame([['brick', 'use as a paperweight'], ['rope', 'dip the end in sugar and use to lure a raccoon closer to you']], columns=['prompt', 'response'])\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RiUajPFuF9m"
      },
      "source": [
        "Here's how that DataFrame may be scored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ieBwOuuYuF9n",
        "outputId": "a7060207-5812-4abc-8902-db90cc5ce01a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  4.42it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>prompt</th>\n",
              "      <th>response</th>\n",
              "      <th>scores</th>\n",
              "      <th>model</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>brick</td>\n",
              "      <td>use as a paperweight</td>\n",
              "      <td>1.5</td>\n",
              "      <td>davinci</td>\n",
              "      <td>1.666815e+09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rope</td>\n",
              "      <td>dip the end in sugar and use to lure a raccoon...</td>\n",
              "      <td>4.1</td>\n",
              "      <td>davinci</td>\n",
              "      <td>1.666815e+09</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  prompt                                           response  scores    model  \\\n",
              "0  brick                               use as a paperweight     1.5  davinci   \n",
              "1   rope  dip the end in sugar and use to lure a raccoon...     4.1  davinci   \n",
              "\n",
              "      timestamp  \n",
              "0  1.666815e+09  \n",
              "1  1.666815e+09  "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['scores'] = scorer.originality_batch(df.prompt, df.response, model='babbage')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('ocstest')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "23c8637676406ba2ac52314b4f9098abb1673d6525a999746607efafd3e21141"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
